{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f8fe0d7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from numpy import genfromtxt # Used to bring data form the web\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0dd15010",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = genfromtxt('boston_housing.csv', delimiter=',') #: WHEN GETTING DATA FROM THE INTERNET\n",
    "\n",
    "data = pd.read_csv('boston_housing.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5b4be970",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 505 entries, 0 to 504\n",
      "Columns: 105 entries, 0.000000000000000000e+00 to 2.400000000000000000e+01\n",
      "dtypes: float64(105)\n",
      "memory usage: 414.4 KB\n"
     ]
    }
   ],
   "source": [
    "data.info()\n",
    "\n",
    "# Data has no column title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "beeef489",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0.000000000000000000e+00</th>\n",
       "      <th>1.799999999999999933e-01</th>\n",
       "      <th>6.781524926686217924e-02</th>\n",
       "      <th>0.000000000000000000e+00.1</th>\n",
       "      <th>3.148148148148147696e-01</th>\n",
       "      <th>5.775052692086607431e-01</th>\n",
       "      <th>6.416065911431514390e-01</th>\n",
       "      <th>2.692031390664641477e-01</th>\n",
       "      <th>0.000000000000000000e+00.2</th>\n",
       "      <th>2.080152671755724492e-01</th>\n",
       "      <th>...</th>\n",
       "      <th>4.327035137812478699e-02</th>\n",
       "      <th>5.974906610362189924e-02</th>\n",
       "      <th>2.080152671755724214e-01</th>\n",
       "      <th>1.865479079251132835e-02</th>\n",
       "      <th>8.250339520144869820e-02</th>\n",
       "      <th>2.872340425531915709e-01</th>\n",
       "      <th>2.575912357334086619e-02</th>\n",
       "      <th>9.999999999999997780e-01</th>\n",
       "      <th>8.967991169977924948e-02</th>\n",
       "      <th>8.042486562480206241e-03</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000236</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.242302</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.172840</td>\n",
       "      <td>0.547998</td>\n",
       "      <td>0.782698</td>\n",
       "      <td>0.348962</td>\n",
       "      <td>0.043478</td>\n",
       "      <td>0.104962</td>\n",
       "      <td>...</td>\n",
       "      <td>0.011017</td>\n",
       "      <td>0.058064</td>\n",
       "      <td>0.104962</td>\n",
       "      <td>0.021462</td>\n",
       "      <td>0.306021</td>\n",
       "      <td>0.553191</td>\n",
       "      <td>0.113111</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.204470</td>\n",
       "      <td>0.041808</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000236</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.242302</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.172840</td>\n",
       "      <td>0.694386</td>\n",
       "      <td>0.599382</td>\n",
       "      <td>0.348962</td>\n",
       "      <td>0.043478</td>\n",
       "      <td>0.104962</td>\n",
       "      <td>...</td>\n",
       "      <td>0.011017</td>\n",
       "      <td>0.058064</td>\n",
       "      <td>0.103885</td>\n",
       "      <td>0.006661</td>\n",
       "      <td>0.306021</td>\n",
       "      <td>0.547514</td>\n",
       "      <td>0.035109</td>\n",
       "      <td>0.979580</td>\n",
       "      <td>0.062814</td>\n",
       "      <td>0.004028</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000293</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.063050</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.150206</td>\n",
       "      <td>0.658555</td>\n",
       "      <td>0.441813</td>\n",
       "      <td>0.448545</td>\n",
       "      <td>0.086957</td>\n",
       "      <td>0.066794</td>\n",
       "      <td>...</td>\n",
       "      <td>0.004461</td>\n",
       "      <td>0.043345</td>\n",
       "      <td>0.066412</td>\n",
       "      <td>0.002230</td>\n",
       "      <td>0.421118</td>\n",
       "      <td>0.645222</td>\n",
       "      <td>0.021667</td>\n",
       "      <td>0.988585</td>\n",
       "      <td>0.033197</td>\n",
       "      <td>0.001115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000705</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.063050</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.150206</td>\n",
       "      <td>0.687105</td>\n",
       "      <td>0.528321</td>\n",
       "      <td>0.448545</td>\n",
       "      <td>0.086957</td>\n",
       "      <td>0.066794</td>\n",
       "      <td>...</td>\n",
       "      <td>0.004461</td>\n",
       "      <td>0.043345</td>\n",
       "      <td>0.066794</td>\n",
       "      <td>0.006635</td>\n",
       "      <td>0.421118</td>\n",
       "      <td>0.648936</td>\n",
       "      <td>0.064464</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.099338</td>\n",
       "      <td>0.009868</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000264</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.063050</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.150206</td>\n",
       "      <td>0.549722</td>\n",
       "      <td>0.574665</td>\n",
       "      <td>0.448545</td>\n",
       "      <td>0.086957</td>\n",
       "      <td>0.066794</td>\n",
       "      <td>...</td>\n",
       "      <td>0.004461</td>\n",
       "      <td>0.043345</td>\n",
       "      <td>0.066326</td>\n",
       "      <td>0.006414</td>\n",
       "      <td>0.421118</td>\n",
       "      <td>0.644387</td>\n",
       "      <td>0.062315</td>\n",
       "      <td>0.986029</td>\n",
       "      <td>0.095353</td>\n",
       "      <td>0.009221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>500</th>\n",
       "      <td>0.000633</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.420455</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.386831</td>\n",
       "      <td>0.580954</td>\n",
       "      <td>0.681771</td>\n",
       "      <td>0.122671</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.164122</td>\n",
       "      <td>...</td>\n",
       "      <td>0.026936</td>\n",
       "      <td>0.146662</td>\n",
       "      <td>0.162090</td>\n",
       "      <td>0.035958</td>\n",
       "      <td>0.798551</td>\n",
       "      <td>0.882553</td>\n",
       "      <td>0.195787</td>\n",
       "      <td>0.975392</td>\n",
       "      <td>0.216382</td>\n",
       "      <td>0.048003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>501</th>\n",
       "      <td>0.000438</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.420455</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.386831</td>\n",
       "      <td>0.490324</td>\n",
       "      <td>0.760041</td>\n",
       "      <td>0.105293</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.164122</td>\n",
       "      <td>...</td>\n",
       "      <td>0.026936</td>\n",
       "      <td>0.146662</td>\n",
       "      <td>0.164122</td>\n",
       "      <td>0.033286</td>\n",
       "      <td>0.798551</td>\n",
       "      <td>0.893617</td>\n",
       "      <td>0.181239</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.202815</td>\n",
       "      <td>0.041134</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>502</th>\n",
       "      <td>0.000612</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.420455</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.386831</td>\n",
       "      <td>0.654340</td>\n",
       "      <td>0.907312</td>\n",
       "      <td>0.094381</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.164122</td>\n",
       "      <td>...</td>\n",
       "      <td>0.026936</td>\n",
       "      <td>0.146662</td>\n",
       "      <td>0.164122</td>\n",
       "      <td>0.017707</td>\n",
       "      <td>0.798551</td>\n",
       "      <td>0.893617</td>\n",
       "      <td>0.096414</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.107892</td>\n",
       "      <td>0.011641</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>503</th>\n",
       "      <td>0.001161</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.420455</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.386831</td>\n",
       "      <td>0.619467</td>\n",
       "      <td>0.889804</td>\n",
       "      <td>0.114514</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.164122</td>\n",
       "      <td>...</td>\n",
       "      <td>0.026936</td>\n",
       "      <td>0.146662</td>\n",
       "      <td>0.162694</td>\n",
       "      <td>0.021512</td>\n",
       "      <td>0.798551</td>\n",
       "      <td>0.885843</td>\n",
       "      <td>0.117127</td>\n",
       "      <td>0.982677</td>\n",
       "      <td>0.129930</td>\n",
       "      <td>0.017180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>504</th>\n",
       "      <td>0.000462</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.420455</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.386831</td>\n",
       "      <td>0.473079</td>\n",
       "      <td>0.802266</td>\n",
       "      <td>0.125072</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.164122</td>\n",
       "      <td>...</td>\n",
       "      <td>0.026936</td>\n",
       "      <td>0.146662</td>\n",
       "      <td>0.164122</td>\n",
       "      <td>0.027852</td>\n",
       "      <td>0.798551</td>\n",
       "      <td>0.893617</td>\n",
       "      <td>0.151649</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.169702</td>\n",
       "      <td>0.028799</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>505 rows × 104 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     0.000000000000000000e+00  1.799999999999999933e-01  \\\n",
       "0                    0.000236                       0.0   \n",
       "1                    0.000236                       0.0   \n",
       "2                    0.000293                       0.0   \n",
       "3                    0.000705                       0.0   \n",
       "4                    0.000264                       0.0   \n",
       "..                        ...                       ...   \n",
       "500                  0.000633                       0.0   \n",
       "501                  0.000438                       0.0   \n",
       "502                  0.000612                       0.0   \n",
       "503                  0.001161                       0.0   \n",
       "504                  0.000462                       0.0   \n",
       "\n",
       "     6.781524926686217924e-02  0.000000000000000000e+00.1  \\\n",
       "0                    0.242302                         0.0   \n",
       "1                    0.242302                         0.0   \n",
       "2                    0.063050                         0.0   \n",
       "3                    0.063050                         0.0   \n",
       "4                    0.063050                         0.0   \n",
       "..                        ...                         ...   \n",
       "500                  0.420455                         0.0   \n",
       "501                  0.420455                         0.0   \n",
       "502                  0.420455                         0.0   \n",
       "503                  0.420455                         0.0   \n",
       "504                  0.420455                         0.0   \n",
       "\n",
       "     3.148148148148147696e-01  5.775052692086607431e-01  \\\n",
       "0                    0.172840                  0.547998   \n",
       "1                    0.172840                  0.694386   \n",
       "2                    0.150206                  0.658555   \n",
       "3                    0.150206                  0.687105   \n",
       "4                    0.150206                  0.549722   \n",
       "..                        ...                       ...   \n",
       "500                  0.386831                  0.580954   \n",
       "501                  0.386831                  0.490324   \n",
       "502                  0.386831                  0.654340   \n",
       "503                  0.386831                  0.619467   \n",
       "504                  0.386831                  0.473079   \n",
       "\n",
       "     6.416065911431514390e-01  2.692031390664641477e-01  \\\n",
       "0                    0.782698                  0.348962   \n",
       "1                    0.599382                  0.348962   \n",
       "2                    0.441813                  0.448545   \n",
       "3                    0.528321                  0.448545   \n",
       "4                    0.574665                  0.448545   \n",
       "..                        ...                       ...   \n",
       "500                  0.681771                  0.122671   \n",
       "501                  0.760041                  0.105293   \n",
       "502                  0.907312                  0.094381   \n",
       "503                  0.889804                  0.114514   \n",
       "504                  0.802266                  0.125072   \n",
       "\n",
       "     0.000000000000000000e+00.2  2.080152671755724492e-01  ...  \\\n",
       "0                      0.043478                  0.104962  ...   \n",
       "1                      0.043478                  0.104962  ...   \n",
       "2                      0.086957                  0.066794  ...   \n",
       "3                      0.086957                  0.066794  ...   \n",
       "4                      0.086957                  0.066794  ...   \n",
       "..                          ...                       ...  ...   \n",
       "500                    0.000000                  0.164122  ...   \n",
       "501                    0.000000                  0.164122  ...   \n",
       "502                    0.000000                  0.164122  ...   \n",
       "503                    0.000000                  0.164122  ...   \n",
       "504                    0.000000                  0.164122  ...   \n",
       "\n",
       "     4.327035137812478699e-02  5.974906610362189924e-02  \\\n",
       "0                    0.011017                  0.058064   \n",
       "1                    0.011017                  0.058064   \n",
       "2                    0.004461                  0.043345   \n",
       "3                    0.004461                  0.043345   \n",
       "4                    0.004461                  0.043345   \n",
       "..                        ...                       ...   \n",
       "500                  0.026936                  0.146662   \n",
       "501                  0.026936                  0.146662   \n",
       "502                  0.026936                  0.146662   \n",
       "503                  0.026936                  0.146662   \n",
       "504                  0.026936                  0.146662   \n",
       "\n",
       "     2.080152671755724214e-01  1.865479079251132835e-02  \\\n",
       "0                    0.104962                  0.021462   \n",
       "1                    0.103885                  0.006661   \n",
       "2                    0.066412                  0.002230   \n",
       "3                    0.066794                  0.006635   \n",
       "4                    0.066326                  0.006414   \n",
       "..                        ...                       ...   \n",
       "500                  0.162090                  0.035958   \n",
       "501                  0.164122                  0.033286   \n",
       "502                  0.164122                  0.017707   \n",
       "503                  0.162694                  0.021512   \n",
       "504                  0.164122                  0.027852   \n",
       "\n",
       "     8.250339520144869820e-02  2.872340425531915709e-01  \\\n",
       "0                    0.306021                  0.553191   \n",
       "1                    0.306021                  0.547514   \n",
       "2                    0.421118                  0.645222   \n",
       "3                    0.421118                  0.648936   \n",
       "4                    0.421118                  0.644387   \n",
       "..                        ...                       ...   \n",
       "500                  0.798551                  0.882553   \n",
       "501                  0.798551                  0.893617   \n",
       "502                  0.798551                  0.893617   \n",
       "503                  0.798551                  0.885843   \n",
       "504                  0.798551                  0.893617   \n",
       "\n",
       "     2.575912357334086619e-02  9.999999999999997780e-01  \\\n",
       "0                    0.113111                  1.000000   \n",
       "1                    0.035109                  0.979580   \n",
       "2                    0.021667                  0.988585   \n",
       "3                    0.064464                  1.000000   \n",
       "4                    0.062315                  0.986029   \n",
       "..                        ...                       ...   \n",
       "500                  0.195787                  0.975392   \n",
       "501                  0.181239                  1.000000   \n",
       "502                  0.096414                  1.000000   \n",
       "503                  0.117127                  0.982677   \n",
       "504                  0.151649                  1.000000   \n",
       "\n",
       "     8.967991169977924948e-02  8.042486562480206241e-03  \n",
       "0                    0.204470                  0.041808  \n",
       "1                    0.062814                  0.004028  \n",
       "2                    0.033197                  0.001115  \n",
       "3                    0.099338                  0.009868  \n",
       "4                    0.095353                  0.009221  \n",
       "..                        ...                       ...  \n",
       "500                  0.216382                  0.048003  \n",
       "501                  0.202815                  0.041134  \n",
       "502                  0.107892                  0.011641  \n",
       "503                  0.129930                  0.017180  \n",
       "504                  0.169702                  0.028799  \n",
       "\n",
       "[505 rows x 104 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.iloc[:, :-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2402e20b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If the data was iported using the numpy genfromtxt, I'd access thecolumns without \".iloc\" because the imported data won't be in datafram\n",
    "\n",
    "x = data.iloc[:, :-1] # Selct all rows and columns except the last one which is the target value\n",
    "y = data.iloc[:, -1] # Y is the dependent value we want to predict\n",
    "\n",
    "#  Refer to 'https://www.geeksforgeeks.org/implementation-of-lasso-ridge-and-elastic-net/' for more insights\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "055282a7",
   "metadata": {},
   "source": [
    "## Split into test and train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b4dce671",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x,y,\n",
    "                                                    test_size=0.25,\n",
    "                                                    random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0304aaa",
   "metadata": {},
   "source": [
    "## Build LinearRegression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f72d5be9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build model\n",
    "\n",
    "lr = LinearRegression().fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1acb7c0",
   "metadata": {},
   "source": [
    "#####  Compare actual y value with the predicted y values in x_test dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "39b98274",
   "metadata": {},
   "outputs": [],
   "source": [
    "predy = lr.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ef979b2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create new columns in the x_test dataframe to have the values compared\n",
    "x_test['ypred'] = predy\n",
    "x_test['yTest'] = y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0e7e49a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0.000000000000000000e+00</th>\n",
       "      <th>1.799999999999999933e-01</th>\n",
       "      <th>6.781524926686217924e-02</th>\n",
       "      <th>0.000000000000000000e+00.1</th>\n",
       "      <th>3.148148148148147696e-01</th>\n",
       "      <th>5.775052692086607431e-01</th>\n",
       "      <th>6.416065911431514390e-01</th>\n",
       "      <th>2.692031390664641477e-01</th>\n",
       "      <th>0.000000000000000000e+00.2</th>\n",
       "      <th>2.080152671755724492e-01</th>\n",
       "      <th>...</th>\n",
       "      <th>2.080152671755724214e-01</th>\n",
       "      <th>1.865479079251132835e-02</th>\n",
       "      <th>8.250339520144869820e-02</th>\n",
       "      <th>2.872340425531915709e-01</th>\n",
       "      <th>2.575912357334086619e-02</th>\n",
       "      <th>9.999999999999997780e-01</th>\n",
       "      <th>8.967991169977924948e-02</th>\n",
       "      <th>8.042486562480206241e-03</th>\n",
       "      <th>ypred</th>\n",
       "      <th>yTest</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>0.000371</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.108138</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.213992</td>\n",
       "      <td>0.544932</td>\n",
       "      <td>0.731205</td>\n",
       "      <td>0.178459</td>\n",
       "      <td>0.043478</td>\n",
       "      <td>0.158397</td>\n",
       "      <td>...</td>\n",
       "      <td>0.157059</td>\n",
       "      <td>0.028279</td>\n",
       "      <td>0.306021</td>\n",
       "      <td>0.548519</td>\n",
       "      <td>0.098762</td>\n",
       "      <td>0.983177</td>\n",
       "      <td>0.177024</td>\n",
       "      <td>0.031874</td>\n",
       "      <td>24.769281</td>\n",
       "      <td>22.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>0.000849</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.089076</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.123457</td>\n",
       "      <td>0.816057</td>\n",
       "      <td>0.350154</td>\n",
       "      <td>0.215115</td>\n",
       "      <td>0.043478</td>\n",
       "      <td>0.169847</td>\n",
       "      <td>...</td>\n",
       "      <td>0.168404</td>\n",
       "      <td>0.008624</td>\n",
       "      <td>0.330014</td>\n",
       "      <td>0.569586</td>\n",
       "      <td>0.029167</td>\n",
       "      <td>0.983077</td>\n",
       "      <td>0.050341</td>\n",
       "      <td>0.002578</td>\n",
       "      <td>40.945906</td>\n",
       "      <td>43.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>477</th>\n",
       "      <td>0.114945</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.646628</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.471193</td>\n",
       "      <td>0.502778</td>\n",
       "      <td>0.966014</td>\n",
       "      <td>0.094654</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.914122</td>\n",
       "      <td>...</td>\n",
       "      <td>0.874476</td>\n",
       "      <td>0.411153</td>\n",
       "      <td>0.653689</td>\n",
       "      <td>0.773445</td>\n",
       "      <td>0.363651</td>\n",
       "      <td>0.915139</td>\n",
       "      <td>0.430272</td>\n",
       "      <td>0.202301</td>\n",
       "      <td>17.763984</td>\n",
       "      <td>14.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>340</th>\n",
       "      <td>0.000075</td>\n",
       "      <td>0.35</td>\n",
       "      <td>0.038856</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.117284</td>\n",
       "      <td>0.705116</td>\n",
       "      <td>0.477858</td>\n",
       "      <td>0.537270</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.185115</td>\n",
       "      <td>...</td>\n",
       "      <td>0.184106</td>\n",
       "      <td>0.019206</td>\n",
       "      <td>0.095179</td>\n",
       "      <td>0.306830</td>\n",
       "      <td>0.032009</td>\n",
       "      <td>0.989137</td>\n",
       "      <td>0.103188</td>\n",
       "      <td>0.010765</td>\n",
       "      <td>29.275967</td>\n",
       "      <td>32.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>395</th>\n",
       "      <td>0.065929</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.646628</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.633745</td>\n",
       "      <td>0.544932</td>\n",
       "      <td>0.958805</td>\n",
       "      <td>0.049759</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.914122</td>\n",
       "      <td>...</td>\n",
       "      <td>0.914122</td>\n",
       "      <td>0.444953</td>\n",
       "      <td>0.653689</td>\n",
       "      <td>0.808511</td>\n",
       "      <td>0.393547</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.486755</td>\n",
       "      <td>0.236930</td>\n",
       "      <td>15.195409</td>\n",
       "      <td>12.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134</th>\n",
       "      <td>0.006198</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.785557</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.491770</td>\n",
       "      <td>0.531519</td>\n",
       "      <td>0.981462</td>\n",
       "      <td>0.089216</td>\n",
       "      <td>0.130435</td>\n",
       "      <td>0.477099</td>\n",
       "      <td>...</td>\n",
       "      <td>0.474416</td>\n",
       "      <td>0.200503</td>\n",
       "      <td>0.837030</td>\n",
       "      <td>0.909749</td>\n",
       "      <td>0.384488</td>\n",
       "      <td>0.988785</td>\n",
       "      <td>0.417891</td>\n",
       "      <td>0.176613</td>\n",
       "      <td>17.193556</td>\n",
       "      <td>18.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>225</th>\n",
       "      <td>0.004224</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.210411</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.244856</td>\n",
       "      <td>0.858210</td>\n",
       "      <td>0.860968</td>\n",
       "      <td>0.189699</td>\n",
       "      <td>0.304348</td>\n",
       "      <td>0.229008</td>\n",
       "      <td>...</td>\n",
       "      <td>0.223510</td>\n",
       "      <td>0.008847</td>\n",
       "      <td>0.260751</td>\n",
       "      <td>0.498380</td>\n",
       "      <td>0.019727</td>\n",
       "      <td>0.952566</td>\n",
       "      <td>0.037704</td>\n",
       "      <td>0.001492</td>\n",
       "      <td>42.289548</td>\n",
       "      <td>37.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>402</th>\n",
       "      <td>0.278694</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.646628</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.633745</td>\n",
       "      <td>0.342594</td>\n",
       "      <td>0.958805</td>\n",
       "      <td>0.052124</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.914122</td>\n",
       "      <td>...</td>\n",
       "      <td>0.914122</td>\n",
       "      <td>0.455043</td>\n",
       "      <td>0.653689</td>\n",
       "      <td>0.808511</td>\n",
       "      <td>0.402471</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.497792</td>\n",
       "      <td>0.247797</td>\n",
       "      <td>10.514661</td>\n",
       "      <td>8.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>493</th>\n",
       "      <td>0.003071</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.338343</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.411523</td>\n",
       "      <td>0.453152</td>\n",
       "      <td>0.408857</td>\n",
       "      <td>0.113859</td>\n",
       "      <td>0.217391</td>\n",
       "      <td>0.389313</td>\n",
       "      <td>...</td>\n",
       "      <td>0.389313</td>\n",
       "      <td>0.127408</td>\n",
       "      <td>0.492983</td>\n",
       "      <td>0.702128</td>\n",
       "      <td>0.229780</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.327263</td>\n",
       "      <td>0.107101</td>\n",
       "      <td>23.692799</td>\n",
       "      <td>24.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>0.001607</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.171188</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.139918</td>\n",
       "      <td>0.417705</td>\n",
       "      <td>0.651905</td>\n",
       "      <td>0.554320</td>\n",
       "      <td>0.304348</td>\n",
       "      <td>0.185115</td>\n",
       "      <td>...</td>\n",
       "      <td>0.184279</td>\n",
       "      <td>0.058334</td>\n",
       "      <td>0.570507</td>\n",
       "      <td>0.751910</td>\n",
       "      <td>0.238017</td>\n",
       "      <td>0.990993</td>\n",
       "      <td>0.313699</td>\n",
       "      <td>0.099302</td>\n",
       "      <td>16.418096</td>\n",
       "      <td>18.7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>127 rows × 106 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     0.000000000000000000e+00  1.799999999999999933e-01  \\\n",
       "90                   0.000371                      0.00   \n",
       "97                   0.000849                      0.00   \n",
       "477                  0.114945                      0.00   \n",
       "340                  0.000075                      0.35   \n",
       "395                  0.065929                      0.00   \n",
       "..                        ...                       ...   \n",
       "134                  0.006198                      0.00   \n",
       "225                  0.004224                      0.00   \n",
       "402                  0.278694                      0.00   \n",
       "493                  0.003071                      0.00   \n",
       "59                   0.001607                      0.25   \n",
       "\n",
       "     6.781524926686217924e-02  0.000000000000000000e+00.1  \\\n",
       "90                   0.108138                         0.0   \n",
       "97                   0.089076                         0.0   \n",
       "477                  0.646628                         0.0   \n",
       "340                  0.038856                         0.0   \n",
       "395                  0.646628                         0.0   \n",
       "..                        ...                         ...   \n",
       "134                  0.785557                         0.0   \n",
       "225                  0.210411                         0.0   \n",
       "402                  0.646628                         0.0   \n",
       "493                  0.338343                         0.0   \n",
       "59                   0.171188                         0.0   \n",
       "\n",
       "     3.148148148148147696e-01  5.775052692086607431e-01  \\\n",
       "90                   0.213992                  0.544932   \n",
       "97                   0.123457                  0.816057   \n",
       "477                  0.471193                  0.502778   \n",
       "340                  0.117284                  0.705116   \n",
       "395                  0.633745                  0.544932   \n",
       "..                        ...                       ...   \n",
       "134                  0.491770                  0.531519   \n",
       "225                  0.244856                  0.858210   \n",
       "402                  0.633745                  0.342594   \n",
       "493                  0.411523                  0.453152   \n",
       "59                   0.139918                  0.417705   \n",
       "\n",
       "     6.416065911431514390e-01  2.692031390664641477e-01  \\\n",
       "90                   0.731205                  0.178459   \n",
       "97                   0.350154                  0.215115   \n",
       "477                  0.966014                  0.094654   \n",
       "340                  0.477858                  0.537270   \n",
       "395                  0.958805                  0.049759   \n",
       "..                        ...                       ...   \n",
       "134                  0.981462                  0.089216   \n",
       "225                  0.860968                  0.189699   \n",
       "402                  0.958805                  0.052124   \n",
       "493                  0.408857                  0.113859   \n",
       "59                   0.651905                  0.554320   \n",
       "\n",
       "     0.000000000000000000e+00.2  2.080152671755724492e-01  ...  \\\n",
       "90                     0.043478                  0.158397  ...   \n",
       "97                     0.043478                  0.169847  ...   \n",
       "477                    1.000000                  0.914122  ...   \n",
       "340                    0.000000                  0.185115  ...   \n",
       "395                    1.000000                  0.914122  ...   \n",
       "..                          ...                       ...  ...   \n",
       "134                    0.130435                  0.477099  ...   \n",
       "225                    0.304348                  0.229008  ...   \n",
       "402                    1.000000                  0.914122  ...   \n",
       "493                    0.217391                  0.389313  ...   \n",
       "59                     0.304348                  0.185115  ...   \n",
       "\n",
       "     2.080152671755724214e-01  1.865479079251132835e-02  \\\n",
       "90                   0.157059                  0.028279   \n",
       "97                   0.168404                  0.008624   \n",
       "477                  0.874476                  0.411153   \n",
       "340                  0.184106                  0.019206   \n",
       "395                  0.914122                  0.444953   \n",
       "..                        ...                       ...   \n",
       "134                  0.474416                  0.200503   \n",
       "225                  0.223510                  0.008847   \n",
       "402                  0.914122                  0.455043   \n",
       "493                  0.389313                  0.127408   \n",
       "59                   0.184279                  0.058334   \n",
       "\n",
       "     8.250339520144869820e-02  2.872340425531915709e-01  \\\n",
       "90                   0.306021                  0.548519   \n",
       "97                   0.330014                  0.569586   \n",
       "477                  0.653689                  0.773445   \n",
       "340                  0.095179                  0.306830   \n",
       "395                  0.653689                  0.808511   \n",
       "..                        ...                       ...   \n",
       "134                  0.837030                  0.909749   \n",
       "225                  0.260751                  0.498380   \n",
       "402                  0.653689                  0.808511   \n",
       "493                  0.492983                  0.702128   \n",
       "59                   0.570507                  0.751910   \n",
       "\n",
       "     2.575912357334086619e-02  9.999999999999997780e-01  \\\n",
       "90                   0.098762                  0.983177   \n",
       "97                   0.029167                  0.983077   \n",
       "477                  0.363651                  0.915139   \n",
       "340                  0.032009                  0.989137   \n",
       "395                  0.393547                  1.000000   \n",
       "..                        ...                       ...   \n",
       "134                  0.384488                  0.988785   \n",
       "225                  0.019727                  0.952566   \n",
       "402                  0.402471                  1.000000   \n",
       "493                  0.229780                  1.000000   \n",
       "59                   0.238017                  0.990993   \n",
       "\n",
       "     8.967991169977924948e-02  8.042486562480206241e-03      ypred  yTest  \n",
       "90                   0.177024                  0.031874  24.769281   22.0  \n",
       "97                   0.050341                  0.002578  40.945906   43.8  \n",
       "477                  0.430272                  0.202301  17.763984   14.6  \n",
       "340                  0.103188                  0.010765  29.275967   32.7  \n",
       "395                  0.486755                  0.236930  15.195409   12.5  \n",
       "..                        ...                       ...        ...    ...  \n",
       "134                  0.417891                  0.176613  17.193556   18.1  \n",
       "225                  0.037704                  0.001492  42.289548   37.6  \n",
       "402                  0.497792                  0.247797  10.514661    8.3  \n",
       "493                  0.327263                  0.107101  23.692799   24.5  \n",
       "59                   0.313699                  0.099302  16.418096   18.7  \n",
       "\n",
       "[127 rows x 106 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cec366c1",
   "metadata": {},
   "source": [
    "## Evaluate Linear regression model score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "01ba433e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set score for LR model: 0.94\n",
      "Test set score for LR model: 0.84\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "training_score = lr.score(x_train, y_train)\n",
    "test_score = lr.score(x_test, y_test)\n",
    "\n",
    "print(f\"Training set score for LR model: {training_score:.2f}\")\n",
    "print(f\"Test set score for LR model: {test_score:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6f406e8",
   "metadata": {},
   "source": [
    "#### This model is overfitted because the difference between the accuracy of the train and test data is high. We will try to improve this by regularization in the subsequent code block using:\n",
    "- Lasso \n",
    "- Ridge regression models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9e7d285",
   "metadata": {},
   "source": [
    "## Regularization with Ridge regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "157aa389",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>Ridge(alpha=0.7)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">Ridge</label><div class=\"sk-toggleable__content\"><pre>Ridge(alpha=0.7)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "Ridge(alpha=0.7)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "# Build ridge model\n",
    "ridge = Ridge(alpha=.7).fit(x_train, y_train)\n",
    "ridge"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3325df9f",
   "metadata": {},
   "source": [
    "##### NB: 'alpha=0.7' is specified arbitrarily\n",
    "##### alpha == lambda == hyperparameter, which needs to be optimized to avoid overfitting or undefitting models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3acc9d0b",
   "metadata": {},
   "source": [
    "## Evaluate Ridge regression model score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "75f07944",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set score for Ridge model: 0.87\n",
      "Test set score for Ridge model: 0.85\n"
     ]
    }
   ],
   "source": [
    "rtraining_score = ridge.score(x_train, y_train)\n",
    "rtest_score = ridge.score(x_test, y_test)\n",
    "\n",
    "print(f\"Training set score for Ridge model: {rtraining_score:.2f}\")\n",
    "print(f\"Test set score for Ridge model: {rtest_score:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b08742fb",
   "metadata": {},
   "source": [
    "####  It is observed that the test score has improved with this Ridge model.\n",
    "\n",
    "Also, the cell below demonstrates how Ridge regression does not reduce the features by setting coefficient to zero. Rather, it reduces the co-efficient of each features but never sets to zero. Hence, the number of features will are still preserved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0005724a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of columns/features: 104\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of columns/features: {sum(ridge.coef_!=0)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55b59f2d",
   "metadata": {},
   "source": [
    "## Regularization with Lasso regression model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "74a6dbe6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>Lasso(alpha=0.9)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">Lasso</label><div class=\"sk-toggleable__content\"><pre>Lasso(alpha=0.9)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "Lasso(alpha=0.9)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "lasso = Lasso(alpha=0.9).fit(x_train, y_train)\n",
    "lasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a3c6d4cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set score for LR model: 0.30\n",
      "Test set score for LR model: 0.29\n"
     ]
    }
   ],
   "source": [
    "ltraining_score = lasso.score(x_train, y_train)\n",
    "ltest_score = lasso.score(x_test, y_test)\n",
    "\n",
    "print(f\"Training set score for LR model: {ltraining_score:.2f}\")\n",
    "print(f\"Test set score for LR model: {ltest_score:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8da64feb",
   "metadata": {},
   "source": [
    "### The model above is underfitted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cb0a85bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of columns/features: 3\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of columns/features: {sum(lasso.coef_!=0)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d26de76",
   "metadata": {},
   "source": [
    "#### Going forward, the hyperparameter for the Lasso model is randomly specified in order to get a more optimized model. \n",
    "#### Lasso model is basically not having to use all the features in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2b13b2c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set score for LR model: 0.87\n",
      "Test set score for LR model: 0.85\n"
     ]
    }
   ],
   "source": [
    "new_lasso = Lasso(alpha=0.01).fit(x_train, y_train)\n",
    "\n",
    "nltraining_score = new_lasso.score(x_train, y_train)\n",
    "nltest_score = new_lasso.score(x_test, y_test)\n",
    "\n",
    "print(f\"Training set score for LR model: {nltraining_score:.2f}\")\n",
    "print(f\"Test set score for LR model: {nltest_score:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ee435bce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of columns/features: 31\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of columns/features: {sum(new_lasso.coef_!=0)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b04e00e",
   "metadata": {},
   "source": [
    "### In order to what hyperparameter makes a better optimized model, I made a loop of the possible values between 0 and 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "45f20756",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.  , 0.02, 0.04, 0.06, 0.08, 0.1 , 0.12, 0.14, 0.16, 0.18, 0.2 ,\n",
       "       0.22, 0.24, 0.26, 0.28, 0.3 , 0.32, 0.34, 0.36, 0.38, 0.4 , 0.42,\n",
       "       0.44, 0.46, 0.48, 0.5 , 0.52, 0.54, 0.56, 0.58, 0.6 , 0.62, 0.64,\n",
       "       0.66, 0.68, 0.7 , 0.72, 0.74, 0.76, 0.78, 0.8 , 0.82, 0.84, 0.86,\n",
       "       0.88, 0.9 , 0.92, 0.94, 0.96, 0.98])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "hyperparameter = np.arange(0, 1, 0.02)\n",
    "hyperparameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9c926c18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.00\n",
      "Training set score for LR model: 0.93\n",
      "Test set score for LR model: 0.85\n",
      "Number of columns/features: 104\n",
      "\n",
      "0.02\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adesa\\AppData\\Local\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:1151: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "C:\\Users\\adesa\\AppData\\Local\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "C:\\Users\\adesa\\AppData\\Local\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.152e+03, tolerance: 3.171e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set score for LR model: 0.85\n",
      "Test set score for LR model: 0.82\n",
      "Number of columns/features: 24\n",
      "\n",
      "0.04\n",
      "Training set score for LR model: 0.82\n",
      "Test set score for LR model: 0.80\n",
      "Number of columns/features: 18\n",
      "\n",
      "0.06\n",
      "Training set score for LR model: 0.79\n",
      "Test set score for LR model: 0.78\n",
      "Number of columns/features: 12\n",
      "\n",
      "0.08\n",
      "Training set score for LR model: 0.76\n",
      "Test set score for LR model: 0.76\n",
      "Number of columns/features: 11\n",
      "\n",
      "0.10\n",
      "Training set score for LR model: 0.73\n",
      "Test set score for LR model: 0.74\n",
      "Number of columns/features: 8\n",
      "\n",
      "0.12\n",
      "Training set score for LR model: 0.73\n",
      "Test set score for LR model: 0.73\n",
      "Number of columns/features: 8\n",
      "\n",
      "0.14\n",
      "Training set score for LR model: 0.72\n",
      "Test set score for LR model: 0.72\n",
      "Number of columns/features: 7\n",
      "\n",
      "0.16\n",
      "Training set score for LR model: 0.71\n",
      "Test set score for LR model: 0.72\n",
      "Number of columns/features: 8\n",
      "\n",
      "0.18\n",
      "Training set score for LR model: 0.71\n",
      "Test set score for LR model: 0.71\n",
      "Number of columns/features: 7\n",
      "\n",
      "0.20\n",
      "Training set score for LR model: 0.70\n",
      "Test set score for LR model: 0.71\n",
      "Number of columns/features: 7\n",
      "\n",
      "0.22\n",
      "Training set score for LR model: 0.70\n",
      "Test set score for LR model: 0.70\n",
      "Number of columns/features: 6\n",
      "\n",
      "0.24\n",
      "Training set score for LR model: 0.69\n",
      "Test set score for LR model: 0.69\n",
      "Number of columns/features: 6\n",
      "\n",
      "0.26\n",
      "Training set score for LR model: 0.69\n",
      "Test set score for LR model: 0.69\n",
      "Number of columns/features: 6\n",
      "\n",
      "0.28\n",
      "Training set score for LR model: 0.68\n",
      "Test set score for LR model: 0.68\n",
      "Number of columns/features: 6\n",
      "\n",
      "0.30\n",
      "Training set score for LR model: 0.67\n",
      "Test set score for LR model: 0.68\n",
      "Number of columns/features: 6\n",
      "\n",
      "0.32\n",
      "Training set score for LR model: 0.66\n",
      "Test set score for LR model: 0.67\n",
      "Number of columns/features: 6\n",
      "\n",
      "0.34\n",
      "Training set score for LR model: 0.66\n",
      "Test set score for LR model: 0.66\n",
      "Number of columns/features: 6\n",
      "\n",
      "0.36\n",
      "Training set score for LR model: 0.65\n",
      "Test set score for LR model: 0.65\n",
      "Number of columns/features: 5\n",
      "\n",
      "0.38\n",
      "Training set score for LR model: 0.64\n",
      "Test set score for LR model: 0.64\n",
      "Number of columns/features: 5\n",
      "\n",
      "0.40\n",
      "Training set score for LR model: 0.63\n",
      "Test set score for LR model: 0.63\n",
      "Number of columns/features: 6\n",
      "\n",
      "0.42\n",
      "Training set score for LR model: 0.62\n",
      "Test set score for LR model: 0.62\n",
      "Number of columns/features: 6\n",
      "\n",
      "0.44\n",
      "Training set score for LR model: 0.61\n",
      "Test set score for LR model: 0.61\n",
      "Number of columns/features: 6\n",
      "\n",
      "0.46\n",
      "Training set score for LR model: 0.60\n",
      "Test set score for LR model: 0.60\n",
      "Number of columns/features: 6\n",
      "\n",
      "0.48\n",
      "Training set score for LR model: 0.59\n",
      "Test set score for LR model: 0.59\n",
      "Number of columns/features: 6\n",
      "\n",
      "0.50\n",
      "Training set score for LR model: 0.57\n",
      "Test set score for LR model: 0.58\n",
      "Number of columns/features: 5\n",
      "\n",
      "0.52\n",
      "Training set score for LR model: 0.56\n",
      "Test set score for LR model: 0.56\n",
      "Number of columns/features: 5\n",
      "\n",
      "0.54\n",
      "Training set score for LR model: 0.55\n",
      "Test set score for LR model: 0.55\n",
      "Number of columns/features: 5\n",
      "\n",
      "0.56\n",
      "Training set score for LR model: 0.54\n",
      "Test set score for LR model: 0.54\n",
      "Number of columns/features: 5\n",
      "\n",
      "0.58\n",
      "Training set score for LR model: 0.52\n",
      "Test set score for LR model: 0.52\n",
      "Number of columns/features: 5\n",
      "\n",
      "0.60\n",
      "Training set score for LR model: 0.51\n",
      "Test set score for LR model: 0.51\n",
      "Number of columns/features: 5\n",
      "\n",
      "0.62\n",
      "Training set score for LR model: 0.49\n",
      "Test set score for LR model: 0.49\n",
      "Number of columns/features: 5\n",
      "\n",
      "0.64\n",
      "Training set score for LR model: 0.48\n",
      "Test set score for LR model: 0.47\n",
      "Number of columns/features: 5\n",
      "\n",
      "0.66\n",
      "Training set score for LR model: 0.46\n",
      "Test set score for LR model: 0.46\n",
      "Number of columns/features: 4\n",
      "\n",
      "0.68\n",
      "Training set score for LR model: 0.45\n",
      "Test set score for LR model: 0.45\n",
      "Number of columns/features: 4\n",
      "\n",
      "0.70\n",
      "Training set score for LR model: 0.44\n",
      "Test set score for LR model: 0.43\n",
      "Number of columns/features: 4\n",
      "\n",
      "0.72\n",
      "Training set score for LR model: 0.42\n",
      "Test set score for LR model: 0.42\n",
      "Number of columns/features: 4\n",
      "\n",
      "0.74\n",
      "Training set score for LR model: 0.41\n",
      "Test set score for LR model: 0.40\n",
      "Number of columns/features: 5\n",
      "\n",
      "0.76\n",
      "Training set score for LR model: 0.39\n",
      "Test set score for LR model: 0.38\n",
      "Number of columns/features: 5\n",
      "\n",
      "0.78\n",
      "Training set score for LR model: 0.38\n",
      "Test set score for LR model: 0.37\n",
      "Number of columns/features: 5\n",
      "\n",
      "0.80\n",
      "Training set score for LR model: 0.36\n",
      "Test set score for LR model: 0.35\n",
      "Number of columns/features: 4\n",
      "\n",
      "0.82\n",
      "Training set score for LR model: 0.35\n",
      "Test set score for LR model: 0.34\n",
      "Number of columns/features: 4\n",
      "\n",
      "0.84\n",
      "Training set score for LR model: 0.34\n",
      "Test set score for LR model: 0.33\n",
      "Number of columns/features: 4\n",
      "\n",
      "0.86\n",
      "Training set score for LR model: 0.32\n",
      "Test set score for LR model: 0.31\n",
      "Number of columns/features: 3\n",
      "\n",
      "0.88\n",
      "Training set score for LR model: 0.31\n",
      "Test set score for LR model: 0.30\n",
      "Number of columns/features: 3\n",
      "\n",
      "0.90\n",
      "Training set score for LR model: 0.30\n",
      "Test set score for LR model: 0.29\n",
      "Number of columns/features: 3\n",
      "\n",
      "0.92\n",
      "Training set score for LR model: 0.29\n",
      "Test set score for LR model: 0.28\n",
      "Number of columns/features: 3\n",
      "\n",
      "0.94\n",
      "Training set score for LR model: 0.28\n",
      "Test set score for LR model: 0.27\n",
      "Number of columns/features: 3\n",
      "\n",
      "0.96\n",
      "Training set score for LR model: 0.27\n",
      "Test set score for LR model: 0.25\n",
      "Number of columns/features: 3\n",
      "\n",
      "0.98\n",
      "Training set score for LR model: 0.25\n",
      "Test set score for LR model: 0.24\n",
      "Number of columns/features: 3\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for h in hyperparameter:\n",
    "    opt_lasso = Lasso(alpha=h).fit(x_train, y_train)\n",
    "    print(f\"{h:.2f}\")\n",
    "    nltraining_score = opt_lasso.score(x_train, y_train)\n",
    "    nltest_score = opt_lasso.score(x_test, y_test)\n",
    "\n",
    "    print(f\"Training set score for LR model: {nltraining_score:.2f}\")\n",
    "    print(f\"Test set score for LR model: {nltest_score:.2f}\")\n",
    "    print(f\"Number of columns/features: {sum(opt_lasso.coef_!=0)}\")\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dabd6a96",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
